{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "594a636f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rajasekhar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV,KFold\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,confusion_matrix,recall_score,precision_score,classification_report,roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import xgboost as xgb\n",
    "# from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from joblib import dump, load\n",
    "import pickle\n",
    "\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "\n",
    "# pip install pandas-profiling \n",
    "# from pandas_profiling import ProfileReport\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ffacdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e5c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ab3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\n",
      "ERROR: No matching distribution found for pickle\n"
     ]
    }
   ],
   "source": [
    "pip install pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21391568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY _TheSpecialOne_  \\\n",
       "0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   scotthamilton   \n",
       "1  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY        mattycus   \n",
       "2  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         ElleCTF   \n",
       "3  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          Karoli   \n",
       "4  0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY        joy_wolf   \n",
       "\n",
       "  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0  is upset that he can't update his Facebook by ...                                                                   \n",
       "1  @Kenichan I dived many times for the ball. Man...                                                                   \n",
       "2    my whole body feels itchy and like its on fire                                                                    \n",
       "3  @nationwideclass no, it's not behaving at all....                                                                   \n",
       "4                      @Kwesidei not the whole crew                                                                    "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing dataset\n",
    "data=pd.read_csv('training.1600000.processed.noemoticon.csv',encoding='ISO-8859-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99e8451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns of dataset\n",
    "data.columns=['sentiment','time','date','query','username','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fcb19999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment        time                          date     query  \\\n",
       "0          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "\n",
       "        username                                               text  \n",
       "0  scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "1       mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "2        ElleCTF    my whole body feels itchy and like its on fire   \n",
       "3         Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "4       joy_wolf                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the dataset after renaming columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db0e31a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599999 entries, 0 to 1599998\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count    Dtype \n",
      "---  ------     --------------    ----- \n",
      " 0   sentiment  1599999 non-null  int64 \n",
      " 1   time       1599999 non-null  int64 \n",
      " 2   date       1599999 non-null  object\n",
      " 3   query      1599999 non-null  object\n",
      " 4   username   1599999 non-null  object\n",
      " 5   text       1599999 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Observing the datatypes and no of null values in each columns\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d1f782",
   "metadata": {},
   "source": [
    "sentiment,time are integer datatype.Remaining all are object datatype.No column is containing null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f57aa59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    799999\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value counts in sentiment column\n",
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bacda3a",
   "metadata": {},
   "source": [
    "sentiment column contains 800000 '4' values & 799999 '0' values (equal distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7dfd7254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Representing sentiment column distribution in barplot\n",
    "data['sentiment'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb76345",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "84852970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  is upset that he can't update his Facebook by ...\n",
       "1          0  @Kenichan I dived many times for the ball. Man...\n",
       "2          0    my whole body feels itchy and like its on fire \n",
       "3          0  @nationwideclass no, it's not behaving at all....\n",
       "4          0                      @Kwesidei not the whole crew "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displaying the data of sentiment & text columns\n",
    "dataset=data[['sentiment','text']]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1096be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the positive sentiment value '4' to '1' for better understanding\n",
    "dataset['sentiment']=dataset['sentiment'].map(lambda x:1 if x==4 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "476c6b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  is upset that he can't update his Facebook by ...\n",
       "1          0  @Kenichan I dived many times for the ball. Man...\n",
       "2          0    my whole body feels itchy and like its on fire \n",
       "3          0  @nationwideclass no, it's not behaving at all....\n",
       "4          0                      @Kwesidei not the whole crew "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displaying the dataset post conversion of positive sentiment value\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c09ae2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing sentiment & text data in list for further processing\n",
    "sentiment,text=list(dataset['sentiment']),list(dataset['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ab4c0",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bab158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined a method with all preprocessing functions\n",
    "\n",
    "def preprocess(textdata):\n",
    "    \n",
    "    # Defining dictionary containing all emojis with their meanings.\n",
    "    emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "              ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "              ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "              ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "              '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "              '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "              ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
    "\n",
    "    stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "                 'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "                 'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "                 'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
    "                 'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "                 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "                 'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "                 'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "                 'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
    "                 's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "                 't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "                 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n",
    "                 'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "                 'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "                 'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "                 \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "    processedText = []\n",
    "        \n",
    "    # Defining regex patterns.\n",
    "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    userPattern       = '@[^\\s]+'\n",
    "    # alphaPattern      = \"[^a-zA-Z0-9]\"\n",
    "    alphaPattern      = \"[^a-zA-Z]\"\n",
    "    sequencePattern   = r\"(.)\\1\\1+\"\n",
    "    seqReplacePattern = r\"\\1\\1\"\n",
    "    \n",
    "    for tweet in textdata:\n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        # Replace all URls with 'URL'\n",
    "        tweet = re.sub(urlPattern,' URL',tweet)\n",
    "        \n",
    "        # Replace all emojis.\n",
    "        for emoji in emojis.keys():\n",
    "            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])    \n",
    "            \n",
    "        # Replace @USERNAME to 'USER'.\n",
    "        tweet = re.sub(userPattern,' USER', tweet)  \n",
    "        \n",
    "        # Replace all non alphabets.\n",
    "        tweet = re.sub(alphaPattern, \" \", tweet)\n",
    "        \n",
    "        # Replace 3 or more consecutive letters by 2 letter.\n",
    "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "        \n",
    "        #Removing punctuations if any left post removing all all non alphabets\n",
    "        all_char_list = []\n",
    "        all_char_list = [char for char in tweet if char not in string.punctuation]\n",
    "        tweet = ''.join(all_char_list)\n",
    "        \n",
    "        # Removing all stopwords as per custom list defined above\n",
    "        tweetwords = ''\n",
    "        for word in tweet.split():\n",
    "            if word not in (stopwordlist):\n",
    "                if len(word)>1:\n",
    "                    # Lemmatizing the word.\n",
    "                    # text_pos = pos_tag(word_tokenize(word))\n",
    "                    # word = lemma.lemmatize(text_pos[0][0],get_wordnet_pos_tag(text_pos[0][1]))\n",
    "                    # word = wordLemm.lemmatize(word)\n",
    "                    tweetwords += (word+' ')\n",
    "            \n",
    "        processedText.append(tweetwords)\n",
    "        \n",
    "    return processedText\n",
    "preprocessedtext = preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eb9fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessedtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f374c74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_tokens=[]\n",
    "for i in preprocessedtext:\n",
    "    tokens=[]\n",
    "    for j in i.split(' '):\n",
    "        tokens.append(j)\n",
    "    after_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c732e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ce790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma=nltk.WordNetLemmatizer()\n",
    "finalpreprocessedtext=[]\n",
    "for i in after_tokens:\n",
    "    text=\" \".join([lemma.lemmatize(word) for word in i])\n",
    "    finalpreprocessedtext.append(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c155ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalpreprocessedtext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef10749",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505d6651",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_neg = finalpreprocessedtext[:800000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7fe5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674304b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_negwords = ' '.join(data_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d041bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating word cloud of negative tweets, where the words appearing larger are more frequent in nature across articles\n",
    "# First 800000 tweets in the list are negative tweets\n",
    "\n",
    "wordcloud = WordCloud(max_words = 1000, width= 1600, height= 800,\n",
    "                      collocations = False).generate(all_negwords)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f55eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating word cloud of positive tweets, where the words appearing larger are more frequent in nature acros articles\n",
    "# Last 800000 tweets in the list are negative tweets\n",
    "\n",
    "data_pos = finalpreprocessedtext[800000:]\n",
    "all_poswords = ' '.join(data_pos)\n",
    "wordcloud = WordCloud(max_words = 1000, width= 1600, height= 800,\n",
    "                      collocations = False).generate(all_negwords)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a method using WhitespaceTokenizer and FreqDist to identify the most frequent set of words in the datset segregated\n",
    "# by target variable\n",
    "\n",
    "token_wspace = tokenize.WhitespaceTokenizer()\n",
    "\n",
    "def mostFrequentWords(tweets,quantity):\n",
    "    all_words = ' '.join(tweets)\n",
    "    all_tokens = token_wspace.tokenize(all_words)\n",
    "    freq_dist = nltk.FreqDist(all_tokens)\n",
    "    df_frequency = pd.DataFrame({\"Word\":list(freq_dist.keys()), \"Frequency\":list(freq_dist.values())})\n",
    "    df_frequency = df_frequency.nlargest(columns=\"Frequency\", n=quantity)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    ax = sns.barplot(data = df_frequency, x = \"Word\", y = \"Frequency\", color = 'blue')\n",
    "    ax.set(ylabel = \"Count\")\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the above method for negative cases, plotting the most frequent top 20 words\n",
    "mostFrequentWords(finalpreprocessedtext[:800000],20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6da2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the above method for positive cases, plotting the most frequent top 20 words\n",
    "mostFrequentWords(finalpreprocessedtext[800000:],20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a7a712",
   "metadata": {},
   "source": [
    "# Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1005bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the entire set into train and test with test size of 5%\n",
    "X_train, X_test, y_train, y_test = train_test_split(finalpreprocessedtext, sentiment,\n",
    "                                                    test_size = 0.05, random_state = 0)\n",
    "print('Data Split done..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371394d",
   "metadata": {},
   "source": [
    "# tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cae7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting TFIDF vectorizer on the train dataset \n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "print('Vectoriser fitted.')\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c605cb9",
   "metadata": {},
   "source": [
    "# Transforming the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9edd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the X_train and X_test dataset on the fitted model\n",
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)\n",
    "print(f'Data Transformed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c2e864",
   "metadata": {},
   "source": [
    "# Creating & evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed834bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a method which will take the model object and then predict on the test set and evaluate on Accuracy, \n",
    "# Confusion Matrix and ROC Curve\n",
    "\n",
    "def model_Evaluate(model):\n",
    "    \n",
    "    # Predict values for Test dataset\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Print the evaluation metrics for the dataset.\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "    # Compute and plot the Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    categories  = ['Negative','Positive']\n",
    "    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n",
    "                xticklabels = categories, yticklabels = categories)\n",
    "    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)\n",
    "    \n",
    "    \n",
    "    plt.title (\"ROC Curve\")\n",
    "    #Area under Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC CURVE')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d312fd",
   "metadata": {},
   "source": [
    "# SVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714aa32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a Linear SVM model with default parameters and then evaluating the model\n",
    "\n",
    "SVCmodel = LinearSVC()\n",
    "SVCmodel.fit(X_train, y_train)\n",
    "model_Evaluate(SVCmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1506af36",
   "metadata": {},
   "source": [
    "# BernouliNB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ef98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a Bernoulli Naive Bayes model with default parameters and then evaluating the model\n",
    "\n",
    "BNBmodel = BernoulliNB(alpha = 2)\n",
    "BNBmodel.fit(X_train, y_train)\n",
    "model_Evaluate(BNBmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ea00c8",
   "metadata": {},
   "source": [
    "# Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae2e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a Logistic Regression model with regularization parameter and max iteration limit and then evaluating the model\n",
    "LRmodel = LogisticRegression(C = 1, max_iter = 1000, n_jobs=-1)\n",
    "LRmodel.fit(X_train, y_train)\n",
    "y_test_pred = model_Evaluate(LRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1752361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the positive and negative sentiment on the test data based on logistic regression model\n",
    "print(\"Positive Sentiment Percentage : {}%\".format(round(np.count_nonzero(y_test_pred == 1)/len(y_test_pred)*100,2)))\n",
    "\n",
    "print(\"Negative Sentiment Percentage : {}%\".format(round(np.count_nonzero(y_test_pred == 0)/len(y_test_pred)*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b10ed15",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b4e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the vectorizer object and Logistic Regression object in pickle files for later use\n",
    "\n",
    "file = open('vectoriser.pickle','wb')\n",
    "pickle.dump(vectoriser, file)\n",
    "file.close()\n",
    "\n",
    "file = open('Sentiment-LR.pickle','wb')\n",
    "pickle.dump(LRmodel, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a0a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename='trained_model.sav'\n",
    "pickle.dump(LRmodel,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4601ce",
   "metadata": {},
   "source": [
    "# Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a method to load the models from pickle file\n",
    "def load_models():  \n",
    "       \n",
    "    # Load the vectoriser.\n",
    "    file = open('vectoriser.pickle', 'rb')\n",
    "    vectoriser = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    # Load the LR Model.\n",
    "    file = open('Sentiment-LR.pickle', 'rb')\n",
    "    LRmodel = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    return vectoriser, LRmodel\n",
    "\n",
    "\n",
    "# Created a method to perform tfidf vectorizer on unseen data and then using the model loaded from pickle file to predict \n",
    "# whether positive or negative and also the probability along with it.\n",
    "# Do note the unseen data should be passed to the model in a list\n",
    "def predict(vectoriser, model, text):\n",
    "    finaldata = []\n",
    "    after_tokens=[]\n",
    "    text=preprocess(text)\n",
    "    for i in text:\n",
    "        tokens=[]\n",
    "        for j in i.split(' '):\n",
    "            tokens.append(j)\n",
    "        after_tokens.append(tokens)\n",
    "    lemma=nltk.WordNetLemmatizer()\n",
    "    finalpreprocessedtext=[]\n",
    "    for i in after_tokens:\n",
    "        text2=\" \".join([lemma.lemmatize(word) for word in i])\n",
    "        finalpreprocessedtext.append(text2)\n",
    "    textdata = vectoriser.transform(finalpreprocessedtext)\n",
    "    sentiment = model.predict(textdata)\n",
    "    \n",
    "    # print(model.classes_)\n",
    "    sentiment_prob = model.predict_proba(textdata)\n",
    "    \n",
    "    for index,tweet in enumerate(text):\n",
    "        if sentiment[index] == 1:\n",
    "            sentiment_probFinal = sentiment_prob[index][1]\n",
    "        else:\n",
    "            sentiment_probFinal = sentiment_prob[index][0]\n",
    "            \n",
    "        sentiment_probFinal2 = \"{}%\".format(round(sentiment_probFinal*100,2))\n",
    "        finaldata.append((tweet, sentiment[index], sentiment_probFinal2))\n",
    "           \n",
    "    # Convert the list into a Pandas DataFrame.\n",
    "    df = pd.DataFrame(finaldata, columns = ['tweet','sentiment', 'Probability(Confidence Level)'])\n",
    "    df = df.replace([0,1], [\"Negative\",\"Positive\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d1685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to classify should be in a list.\n",
    "text = [\"I hate twitter\",\n",
    "        \"I do not like the movie\",\n",
    "        \"Mr. Stark, I don't feel so good\",\n",
    "        \"May the Force be with you.\",\n",
    "       \"I read the book, the content is not good\",\n",
    "       \"This is a new beginning for us\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a5864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the load model method and then calling predict method by passing the tfidf vectorizer and model as parameter\n",
    "# Finally printing the dataframe containing tweet, sentiment and the probability confidence.\n",
    "vectoriser, LRmodel = load_models()\n",
    "df = predict(vectoriser, LRmodel, text)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c65dbf2",
   "metadata": {},
   "source": [
    "# Using the model by giving test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccafd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "data = pd.read_csv(\"testdata.csv\", encoding = \"ISO-8859-1\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93de21b2",
   "metadata": {},
   "source": [
    "# Storing text data in lists for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49722b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b35c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a method to perform tfidf vectorizer on unseen data and then using the model loaded from pickle file to predict \n",
    "# whether positive or negative and also the probability along with it.\n",
    "# Do note the unseen data should be passed to the model in a list\n",
    "def predict_Excel(vectoriser, model, text):\n",
    "    finaldata = []\n",
    "    text=preprocess(text)\n",
    "    for i in text:\n",
    "        tokens=[]\n",
    "        for j in i.split(' '):\n",
    "            tokens.append(j)\n",
    "        after_tokens.append(tokens)\n",
    "    lemma=nltk.WordNetLemmatizer()\n",
    "    finalpreprocessedtext=[]\n",
    "    for i in after_tokens:\n",
    "        text2=\" \".join([lemma.lemmatize(word) for word in i])\n",
    "        finalpreprocessedtext.append(text2)\n",
    "\n",
    "    textdata = vectoriser.transform(finalpreprocessedtext)\n",
    "    sentiment = model.predict(textdata)\n",
    "    \n",
    "    # print(model.classes_)\n",
    "    sentiment_prob = model.predict_proba(textdata)\n",
    "    \n",
    "    for index,tweet in enumerate(text):\n",
    "        sentiment_probFinal = sentiment_prob[index][0]\n",
    "            \n",
    "        sentiment_probFinal2 = \"{}%\".format(round(sentiment_probFinal*100,2))\n",
    "        finaldata.append((tweet, sentiment[index], sentiment_probFinal2))\n",
    "           \n",
    "    # Convert the list into a Pandas DataFrame.\n",
    "    df = pd.DataFrame(finaldata, columns = ['tweet','sentiment', 'Severity'])\n",
    "    df = df.replace([0,1], [\"Negative\",\"Positive\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583eb377",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser, LRmodel = load_models()\n",
    "df = predict_Excel(vectoriser, LRmodel, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['Severity'],ascending=False,inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2146879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = round(np.count_nonzero(df['sentiment'] == \"Positive\")/len(df['sentiment'])*100,2)\n",
    "negative = round(np.count_nonzero(df['sentiment'] == \"Negative\")/len(df['sentiment'])*100,2)\n",
    "\n",
    "labels = ['Positive','Negative']\n",
    "values = np.array([positive,negative])\n",
    "myexplode = [0.2, 0]\n",
    "mycolors = [\"green\", \"red\"]\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.pie(values, labels = labels, explode = myexplode, shadow = True, colors = mycolors)\n",
    "ax.legend()\n",
    "ax.set_title(\"Positive vs Negative Tweet(%)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88b969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
